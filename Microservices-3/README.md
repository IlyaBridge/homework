#  «Создание собственных модулей» "`«Микросервисы: подходы»`" - `Казначеев Илья`

https://github.com/netology-code/micros-homeworks/blob/main/11-microservices-03-approaches.md

Вы работаете в крупной компании, которая строит систему на основе микросервисной архитектуры. Вам как DevOps-специалисту необходимо выдвинуть предложение по организации инфраструктуры для разработки и эксплуатации.

---

# Задача 1: Обеспечить разработку
Предложите решение для обеспечения процесса разработки: хранение исходного кода, непрерывная интеграция и непрерывная поставка. Решение может состоять из одного или нескольких программных продуктов и должно описывать способы и принципы их взаимодействия.

Решение должно соответствовать следующим требованиям:

облачная система;
система контроля версий Git;
репозиторий на каждый сервис;
запуск сборки по событию из системы контроля версий;
запуск сборки по кнопке с указанием параметров;
возможность привязать настройки к каждой сборке;
возможность создания шаблонов для различных конфигураций сборок;
возможность безопасного хранения секретных данных (пароли, ключи доступа);
несколько конфигураций для сборки из одного репозитория;
кастомные шаги при сборке;
собственные докер-образы для сборки проектов;
возможность развернуть агентов сборки на собственных серверах;
возможность параллельного запуска нескольких сборок;
возможность параллельного запуска тестов.
Обоснуйте свой выбор.

# Решение 1
## Предлагаемое решение
Для полного соответствия всем требованиям я предлагаю использовать не один монолитный инструмент, а связку из нескольких продуктов:

1. Система контроля версий (SCM): GitLab Ultimate (или GitLab.com Gold)
2. Платформа для сборки и тестирования (CI/CD): GitLab CI/CD (как часть GitLab)
3. Артефакт-репозиторий / Package Registry: GitLab Package Registry (встроен) + Harbor (опционально для повышенных требований к безопасности)
4. Orchestrator для агентов: Kubernetes (для запуска GitLab Runner)

№№ Взаимодействие я вижу следующим образом:
- Разработчики пушат код в репозитории GitLab.
- GitLab, на основе файла .gitlab-ci.yml в репозитории, триггерит пайплайн.
- GitLab Runner, зарегистрированный в проекте и развернутый в Kubernetes-кластере, подхватывает задания из пайплайна.
- Runner создает Pod в Kubernetes для каждого задания (job), используя указанный Docker-образ.

- В процессе выполнения шагов (scripts) job может пушить собранные артефакты или Docker-образы обратно в GitLab Package Registry или во внешний Harbor.
- Успешная сборка может триггерить следующий этап пайплайна (например, деплой в среду).

## Чем я обосновываю данный выбор
Вместо того чтобы выбирать отдельно Jenkins, GitHub и Drone, GitLab предлагает полностью интегрированное All-in-One решение, которое на мой взгляд соответствует всем пунктам задания.

---

# Задача 2: Логи
Предложите решение для обеспечения сбора и анализа логов сервисов в микросервисной архитектуре. Решение может состоять из одного или нескольких программных продуктов и должно описывать способы и принципы их взаимодействия.

Решение должно соответствовать следующим требованиям:

сбор логов в центральное хранилище со всех хостов, обслуживающих систему;
минимальные требования к приложениям, сбор логов из stdout;
гарантированная доставка логов до центрального хранилища;
обеспечение поиска и фильтрации по записям логов;
обеспечение пользовательского интерфейса с возможностью предоставления доступа разработчикам для поиска по записям логов;
возможность дать ссылку на сохранённый поиск по записям логов.
Обоснуйте свой выбор.

# Решение 2
## Предлагаемое решение
С целью полного соблюдения всех требований, я предлагаю использовать следующую схему:
1. Агент для сбора логов: Fluentd или Fluent Bit
2. Буфер/Брокер сообщений (для гарантированной доставки): Apache Kafka
3. Поисковый движок и хранилище: Elasticsearch
4. Веб-интерфейс для визуализации и анализа: Kibana

## Способ и принципы взаимодействия:
1. Сбор (на каждом хосте/в каждом pod):
- На каждом сервере или в каждом pod Kubernetes в виде sidecar-контейнера запускается агент Fluent Bit.
- Fluent Bit постоянно читает логи приложений из stdout и stderr.
- Агент обогащает записи метаданными (например, service_name=user-service, pod_name=user-service-xyz, environment=production).

2. Буферизация и гарантированная доставка:
- Fluent Bit отправляет структурированные логи не напрямую в хранилище, а в Apache Kafka.
- Kafka выступает в роли высокопроизводительного и надежного буфера. Он принимает логи даже если конечное хранилище (Elasticsearch) недоступно для технического обслуживания или не справляется с нагрузкой.

Это гарантирует, что ни одно логирующее событие не будет потеряно при пиковых нагрузках или сбоях downstream-систем.

3. Индексация и хранение:
- Logstash или Fluentd (в более тяжелой конфигурации) потребляет сообщения из Kafka.
- Он выполняет дополнительную обработку, парсинг (например, извлекает поля из JSON-лога), фильтрацию и преобразование данных.
- После обработки Logstash отправляет данные в Elasticsearch для индексации и долгосрочного хранения.

4. Визуализация и анализ:
- Kibana подключается к индексам Elasticsearch.

Разработчики и DevOps-инженеры используют веб-интерфейс Kibana для поиска, фильтрации, анализа логов и создания дашбордов.

## Вывод
Предложенный стек на основе **Fluent Bit → Kafka → Elasticsearch → Kibana** является отказоустойчивым, высокопроизводительным и полностью соответствующим всем требованиям для крупной компании с микросервисной архитектурой.

---

# Задача 3: Мониторинг
Предложите решение для обеспечения сбора и анализа состояния хостов и сервисов в микросервисной архитектуре. Решение может состоять из одного или нескольких программных продуктов и должно описывать способы и принципы их взаимодействия.

Решение должно соответствовать следующим требованиям:

сбор метрик со всех хостов, обслуживающих систему;
сбор метрик состояния ресурсов хостов: CPU, RAM, HDD, Network;
сбор метрик потребляемых ресурсов для каждого сервиса: CPU, RAM, HDD, Network;
сбор метрик, специфичных для каждого сервиса;
пользовательский интерфейс с возможностью делать запросы и агрегировать информацию;
пользовательский интерфейс с возможностью настраивать различные панели для отслеживания состояния системы.
Обоснуйте свой выбор.

# Решение 3
## Предлагаемое решение: Стек Prometheus + Grafana
Для комплексного мониторинга микросервисной архитектуры предлагается использовать следующую схему:
1. Сбор метрик хостов: Node Exporter
2. Сбор метрик контейнеров: cAdvisor (для Docker/Kubernetes)
3. Сбор кастомных метрик приложений: Prometheus Client Libraries + Exporters
4. База данных метрик и обработка: Prometheus Server
5. Визуализация и дашборды: Grafana
6. Опционально для масштабирования: VictoriaMetrics вместо Prometheus

## Способ и принципы взаимодействия:
1. Сбор метрик (Pull-модель):
- На каждом сервере запускается Node Exporter для сбора метрик хоста (CPU, RAM, Disk, Network)
- В среде Kubernetes cAdvisor автоматически собирает метрики контейнеров
- Каждый микросервис предоставляет метрики через HTTP endpoint (`/metrics`), используя Prometheus-клиент для своего языка

2.  Сбор и хранение:
- Prometheus Server периодически опрашивает (scrape) все targets по расписанию
- Данные хранятся в эффективном формате на диске с временной базой данных

3. Визуализация и анализ:
- Grafana подключается к Prometheus как к источнику данных
- Пользователи создают дашборды с использованием мощного языка запросов PromQL

# Обоснование выбора и соответствие требованиям
| Требование | Реализация в предложенном стеке | Обоснование |
| :--- | :--- | :--- |
| **Сбор метрик со всех хостов** | **Node Exporter** на каждом хосте + **Prometheus Server** | Node Exporter - стандарт де-факто для сбора метрик инфраструктуры |
| **Сбор метрик состояния ресурсов хостов** | **Node Exporter** (node_cpu, node_memory, node_disk, node_network) | Предоставляет полный набор системных метрик в Prometheus-формате |
| **Сбор метрик потребляемых ресурсов для каждого сервиса** | **cAdvisor** (container_cpu_usage, container_memory_usage) | Интегрируется с Docker/Kubernetes, автоматически обнаруживает контейнеры |
| **Сбор метрик, специфичных для каждого сервиса** | **Prometheus Client Libraries** (Python, Go, Java etc.) | Разработчики могут добавлять custom-метрики в код приложения без дополнительных компонентов |
| **Пользовательский интерфейс с запросами и агрегацией** | **Grafana** + **PromQL** | Мощный язык запросов PromQL позволяет делать сложные агрегации и преобразования |
| **Пользовательский интерфейс с настраиваемыми панелями** | **Grafana** с дашбордами | Богатые возможности визуализации, шаблонизация, reusable panels |

## Преимущества предложенного решения:
1. Единая экосистема - все компоненты разработаны для совместной работы
2. Pull-модель - упрощает настройку и обнаружение новых targets
3. Многоязычная поддержка - клиентские библиотеки для всех популярных языков программирования
4. Масштабируемость - возможность установки нескольких Prometheus-серверов для разных окружений
5. Сообщество - большое количество готовых дашбордов и экспортеров
6. Эффективное хранение - специальный формат хранения временных рядов

## Вывод
Стек Prometheus + Grafana является отраслевым стандартом для мониторинга микросервисных архитектур, соответствует всем требованиям и обеспечивает надежный сбор и анализ метрик.





---